{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Find Followees on Twitter\n",
    "\n",
    "## 1. Introduction and Problem Statement\n",
    "\n",
    "Social networking is becoming an indispensable part for us, which has made our lives easier, more efficient and convenient. Users can post and share their ideas with each other on social media, at the same time, get news and updates from different fields. Twitter, a popular and powerful social networking tool which is used by billions people every single day, provides free service to users and connects people from all over the world. As a social media application, it is important to help users find the information they need and follow the people or things they are interested in.\n",
    "\n",
    "Although users can find their \"followees\" from hot topics or their friends, we sometimes cannot find the one we really want to follow because Twitter don't display more enough information about the guys we haven't followed. Now, we'd like to design a twitter recommender system to help users to find the followee they are interested in and may follow in the future.\n",
    "\n",
    "In this project, we implemented content-based and collaborative filtering approaches separately to do user recommendation.  \n",
    "  \n",
    "For the hybrid content-based system, firstly we construct a user mentioning graph by what the user mentions in its tweets. Based on the graph, we calculate a score for target user and common users as a baseline of next step. Then we extract the tweets and analyze their similarities. Finally, we combine them together to get a score list to estimate which user should be recommended.  \n",
    "\n",
    "For the collaborative filtering system, especially in this topic, we cannot get an ideal result from the old CF method with jaccard/cosine to calculate the similarity because the relationship between users is only follow/unfollow (no ratings),  we build a user following graph and do PageRank[1] on the graph to rank the popularity of the nodes. According to the popularity rank, we calculate the similarity between the users and recommend the target user with the followees of his/her similar users.  \n",
    "\n",
    "\n",
    "\n",
    "## 2. Related Work:\n",
    "\n",
    "Collaborative filtering recommendation system is based on the idea that similar users will do similar performance. Early work on collaborative filtering only put same weights on every users and calculate the similarity using Jaccard or Cosine[2]. Later work separate the system into several parts and use hybrid strategy to find the similar users[3]. However, the strategy still regards each user as same node and do not care more about the latent difference among individuals. In this project, we mainly use pageRank to get the popularity difference between users and calculate the similarity based on this.\n",
    "\n",
    "Content-based recommendation systems try to recommend items similar to those a given user has liked in the past.[4] For the early work, the basic process performed by a content-based recommender consists in matching up the attributes of a user profile in which preferences and interests are stored, with the attributes of a content object, in order to recommend to the user new interesting items. Purely content-based is no need for data from other users which means there will be no cold-start problem for it. However, it’s hard for it to find out appropriate features. And due to it, purely content-based recommender always has a lower accuracy than collaborative filtering. So based on the traditional content-based algorithm, we add some new features based on observation and experiments. And by combining with user mentions, we try to promote the recommender even with a limited dataset. \n",
    "\n",
    "\n",
    "## 3. Data Collection:\n",
    "When this project was first conceived, we planned on using twitter as our main source of data. However, don’t know when, twitter began to limit the open resource of tweets. It’s impossible to get completely existing tweets data. And the API of twitter is extremely unfriendly for users with so many limitations. \n",
    "\n",
    "So we can only get user relationship graph from SNAP (for the first part) and the tweets data from homework 2 (for the second part). Even so, the user following data is so huge (around 25GB) to deal with, so we have to extract a small part of it for our algorithm to use. Since the data is from around 4 years ago, so some of users may have deactivated or closed their accounts. That also leads us some troubles. So in the evaluation of part 2, we will not use the those kind of IDs to test our algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Hybrid Content-based Recommend System\n",
    "For the hybrid content-based system, firstly we construct a user mentioning graph by what the user mentions in its tweets. Based on the graph, we calculate a score for target user and common users as a baseline of next step. Then we extract the tweets and analyze their similarities. Finally, we combine them together to get a score list to estimate which user should be recommended.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Construct Mention Graph and Extract Tweets\n",
    "\n",
    "For each user mentioning, we construct a dictionary to store their mentioning relationship. And for the next step, we also extract the user tweets at this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import copy\n",
    "\n",
    "graph = {}\n",
    "mentionedGraph = {}\n",
    "defaultRec = {}\n",
    "baseScore = []\n",
    "privateScore=[]\n",
    "nodeIndex = {}\n",
    "stopList = []\n",
    "id2Name = {}\n",
    "name2ID = {}\n",
    "contentByID = {}\n",
    "#scoreByID = {}\n",
    "\n",
    "tempContent = []\n",
    "f = open('english.stop')\n",
    "for line in f:\n",
    "    tempContent.append(line)\n",
    "f.close()\n",
    "stopList=set(('\\n'.join(tempContent)).split())\n",
    "\n",
    "def constructGraph():\n",
    "    inputFile = open('pagerank.json', 'r')\n",
    "    tweets = inputFile.readlines()\n",
    "    for content in tweets:\n",
    "        data = json.loads(content)\n",
    "        userId = data['user']['id']\n",
    "        name = data['user']['screen_name']\n",
    "        if userId not in id2Name:\n",
    "            id2Name[userId]=name\n",
    "        if name not in name2ID:\n",
    "            name2ID[name]=userId\n",
    "        if userId not in graph.keys():\n",
    "            graph[userId] = []\n",
    "        for mention in data['entities']['user_mentions']:\n",
    "            mentionedName = mention['screen_name']\n",
    "            mentionedID = mention['id']\n",
    "            if mentionedID not in id2Name:\n",
    "                id2Name[mentionedID]=mentionedName\n",
    "            if mentionedName not in name2ID:\n",
    "                name2ID[mentionedName]=mentionedID\n",
    "            if mention['id'] != userId:\n",
    "                if mention['id'] not in graph:\n",
    "                    graph[mention['id']] = []\n",
    "                if mention['id'] not in graph[userId]:\n",
    "                    graph[userId].append(mention['id'])\n",
    "                if mention['id'] in mentionedGraph:\n",
    "                    if userId not in mentionedGraph[mention['id']]:\n",
    "                        mentionedGraph[mention['id']].append(userId)\n",
    "                else:\n",
    "                    mentionedGraph[mention['id']] = [userId]\n",
    "    print 'User Graph loaded'\n",
    "    inputFile.close()\n",
    "#constructGraph()\n",
    "#print graph\n",
    "#for node in graph:\n",
    "#    if len(graph[node])>7:\n",
    "#        print node, graph[node], '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Pagerank and Baseline\n",
    "\n",
    "Based on user mentioning graph, we run the pagerank algorithm to get the baseline score of common recommendation, so we can recommmend for new users. Furthermore, we allow to introduce a traget ID to calculate the private score for that user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PageRanker(targetID):\n",
    "# clean empty nodes\n",
    "    tempSet = set()\n",
    "    for node in graph.keys():\n",
    "        if not graph[node]:\n",
    "            tempSet.add(node)\n",
    "    for node in tempSet:\n",
    "        if node not in mentionedGraph.keys() or not mentionedGraph[node]:\n",
    "            del graph[node]\n",
    "            \n",
    "    # build index of node\n",
    "    num = 0\n",
    "    for node in graph:\n",
    "        nodeIndex[node] = num\n",
    "        num += 1\n",
    "        \n",
    "    # build Transfer Matrix  \n",
    "    length = len(graph)\n",
    "    d = 0.9\n",
    "    linkMat = np.zeros((length, length))\n",
    "    teleMat = np.full((length,length), (1/length))\n",
    "    row = 0\n",
    "    for node in graph.keys():\n",
    "        mentionedList = []\n",
    "        for mentionedNode in graph[node]:\n",
    "            mentionedList.append(nodeIndex[mentionedNode])\n",
    "        if len(mentionedList) == 0:\n",
    "            j = 1/length\n",
    "            for x in range(length):\n",
    "                linkMat[row,x] = j\n",
    "        else:\n",
    "            j = 1/len(mentionedList)\n",
    "            for x in mentionedList:\n",
    "                linkMat[row,x] = j\n",
    "        row += 1\n",
    "\n",
    "    tranMat = d * linkMat + (1-d) * teleMat\n",
    "\n",
    "    #make the default recommendations for new users\n",
    "    # calculate pr in iteration times\n",
    "    #iteration = 200\n",
    "    pr = np.full((1,length),(1/length))\n",
    "    #pr = np.zeros((1,length))\n",
    "    #pr[0][nodeIndex[1514379421]] = 1\n",
    "    c = 1\n",
    "    while c >= 0.1:\n",
    "        pr1 = pr\n",
    "        pr = np.dot(pr,tranMat)\n",
    "        c = abs(np.dot(pr1, pr.T) - np.dot(pr,pr.T))\n",
    "    \n",
    "\n",
    "    # Rank result\n",
    "    global baseScore\n",
    "    result = pr.tolist()[0]\n",
    "    baseScore = copy.deepcopy(result)\n",
    "    scoreList = [0.0 for i in range(5)]\n",
    "    rankList = [0 for i in range(5)]\n",
    "    k = 0\n",
    "    for node in graph.keys():\n",
    "        score = result[k]\n",
    "        for l in range(5):\n",
    "            if score > scoreList[l]:\n",
    "                rankList[l] = node\n",
    "                scoreList[l] = score\n",
    "                break\n",
    "        k += 1\n",
    "    for m in range(5):\n",
    "        defaultRec[rankList[m]] = scoreList[m]\n",
    "        \n",
    "    #begin to calculate the private score\n",
    "    if targetID in nodeIndex:\n",
    "        pr = np.zeros((1,length)) \n",
    "        pr[0][nodeIndex[targetID]] = 1\n",
    "        c = 1\n",
    "        while c >= 0.1:\n",
    "            pr1 = pr\n",
    "            pr = np.dot(pr,tranMat)\n",
    "            c = abs(np.dot(pr1, pr.T) - np.dot(pr,pr.T))\n",
    "    \n",
    "        global privateScore\n",
    "        result = pr.tolist()[0]\n",
    "        privateScore = copy.deepcopy(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Rank result\n",
    "    scoreList1 = [0.0 for i in range(10)]\n",
    "    rankList1 = [0 for i in range(10)]\n",
    "    k = 0\n",
    "    for node in graph.keys():\n",
    "        score = result[k]\n",
    "        for l in range(10):\n",
    "            if score > scoreList1[l]:\n",
    "                rankList1[l] = node\n",
    "                scoreList1[l] = score\n",
    "                break\n",
    "        k += 1\n",
    "    \n",
    "  \n",
    "    # Test Result\n",
    "    print 'user ID - Score'\n",
    "    for m in range(5):\n",
    "        print '%s - %f' %(rankList[m], scoreList[m])\n",
    "    print defaultRec\n",
    "\n",
    "    print 'user ID - Score'\n",
    "    for m in range(10):\n",
    "        print '%s - %f' %(rankList1[m], scoreList1[m])\n",
    "        \n",
    "    \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Content Preprocess\n",
    "\n",
    "In this part, we tokenize user tweets in word. Since we need to figure out the similarity of user contents, so we eliminate stop words and URL in text by regular expression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterStopWords(words):\n",
    "    \"\"\"Filters stop words.\"\"\"\n",
    "    filtered = []\n",
    "    for word in words:\n",
    "        if not word in stopList and word.strip() != '':\n",
    "            filtered.append(word)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def contentBased():\n",
    "    inputFile = open('pagerank.json', 'r')\n",
    "    tweets = inputFile.readlines()\n",
    "    for content in tweets:\n",
    "        data = json.loads(content)\n",
    "        userId = data['user']['id']\n",
    "        words = data['text'].lower()\n",
    "        #text='RT @KissMeTheVamps: @TheVampsBrad  @TheVampsband @TheVampsJames @TheVampsCon @TheVampsBrad @TheVampsTristan http:\\\\/\\\\/t.co\\\\/14cow6BScW MY FAVOR\\u2026'\n",
    "        words = re.sub('(http:\\\\\\\\[\\/\\\\\\.a-z]+)', '', words)\n",
    "        words = filterStopWords(words.split())\n",
    "        if userId not in contentByID:\n",
    "            contentByID[userId] = {}\n",
    "        for word in words:\n",
    "            if word in contentByID[userId]:\n",
    "                contentByID[userId][word] += 1\n",
    "            else:\n",
    "                contentByID[userId][word] = 1\n",
    "    \n",
    "    \n",
    "#use Manhattan distance to calculate \n",
    "\n",
    "\n",
    "#print len(contentByID)\n",
    "#print len(mentionedGraph[493249666]), len(mentionedGraph[618336955]), len(mentionedGraph[30974408]), len(mentionedGraph[1479930294]),len(mentionedGraph[1481176242]),len(mentionedGraph[16510838])\n",
    "#print len(mentionedGraph[1056749292]), len(mentionedGraph[495074577]), len(mentionedGraph[126370504]), len(mentionedGraph[629550615]),len(mentionedGraph[1364831238]),len(mentionedGraph[197759552])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Recommendation Calculation\n",
    "\n",
    "In this part, we tried many method to calculate the similarity such as jaccard and manhattan distance. However, we expect the result can take the times of word appearance into consideration and it should be easy to normalized in a certain range. So we use cosine to determine the similarity of two tweets. After that, we sum up the corresponding user baseline score with their similarity score and store it as part of user profile. Then we adapt the parameters based on observation and experiment result. For the number of followers in certain range, we add additional weight for that user. If users follow each other, they will have higher weight for recommendation. And then we sort the result and output them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Graph loaded\n"
     ]
    }
   ],
   "source": [
    "def cosine(a,b):\n",
    "    tempScore = 0.0\n",
    "    a2 = 0.0\n",
    "    b2 = 0.0\n",
    "    c=list(set(a.keys()+b.keys()))\n",
    "    for word in c:\n",
    "        if word not in a or word not in b:\n",
    "            continue\n",
    "        else:\n",
    "            tempScore += a[word]*b[word]\n",
    "    for a1 in a:\n",
    "        a2 += a[a1]*a[a1]\n",
    "    for b1 in b:\n",
    "        b2 += b[b1]*b[b1]\n",
    "    tempScore = tempScore / a2 / b2\n",
    "    return tempScore\n",
    "\n",
    "\n",
    "def recommend(targetID):\n",
    "    PageRanker(targetID)\n",
    "    if targetID in graph:\n",
    "        scoreByID = {}\n",
    "        for user in nodeIndex:\n",
    "            if user != targetID:\n",
    "                score = baseScore[nodeIndex[user]] + privateScore[nodeIndex[user]]\n",
    "                #use cosine to calculate the score of content-based part \n",
    "                if user in contentByID:\n",
    "                    score += cosine(contentByID[targetID], contentByID[user])\n",
    "                if user in mentionedGraph:\n",
    "                    if len(mentionedGraph[user]) >= 15:\n",
    "                        score += 0.0015\n",
    "                    if len(mentionedGraph[user]) <= 110:\n",
    "                        score += 0.0005\n",
    "                    if len(mentionedGraph[user]) >= 250:\n",
    "                        score += 0.0005\n",
    "                    if len(mentionedGraph[user]) >= 20 and len(mentionedGraph[user]) <= 60:\n",
    "                        score += 0.0005\n",
    "                    if targetID in graph and user in graph[targetID]:\n",
    "                        score += 0.0015\n",
    "                    if user in graph and targetID in graph[user]:\n",
    "                        score += 0.0005\n",
    "                scoreByID[user] = score\n",
    "        scoreList = sorted(scoreByID.items(), key=lambda d:d[1], reverse = True)\n",
    "        print 'Recommendations for user %s,  user ID:%d' %(id2Name[targetID], targetID)\n",
    "    else:\n",
    "        scoreList = sorted(defaultRec.items(), key=lambda d:d[1], reverse = True)\n",
    "        print 'Recommendations for user %d' %targetID\n",
    "    j=0\n",
    "    print 'user ID - User Name'\n",
    "    for node in scoreList:\n",
    "        if j<5:\n",
    "            print '%s - %s' %(node[0], id2Name[node[0]]) \n",
    "            j+=1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "constructGraph()\n",
    "contentBased()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputID = raw_input('Please input the user ID you want to recommend for:')\n",
    "recommend(int(inputID))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Approach and Methods\n",
    "For the second part, we ever thought about several ways to combine the result of two parts. Since in the very beginning, we use manhattan to calculate the similarity and it's actually too hard to contral the range of its score. The first idea is use a coefficient to limit it. However, tweets of users are too different with each other. The length of them drove me crazy. Then we noticed the recommendation part for movies of our class. So we decided to try the baseline thought and it works well. But the normalization problem was still not solved. Actually, when I found that the cosine method had no need to normalize, I thought I was an idiot (the easiest way works the best T_T). Another interesting discovery is that we found that the popular user sometimes didn't followed by target user in our dataset. On the contrary, some common users, whose number of followers was around 50, had higer probability to be followed by another common user. That promotes our precision a lot. \n",
    "\n",
    "As we discussed at the beginning, the biggest problem is the data collection. It's a hard time to search for it and finally found that the twitter had already limited open resource of tweets for several years. And we tried to write script to capture tweets by ourselves. However, twitter is a bad company which didn't like to support research. Its limitation of API is so harsh that we were blocked with only 15 commands. But it's actually not a bad experience, since we learned a lot of how to capture data by ourselves, even which was not very successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Recommendation Result and Evaluation\n",
    "\n",
    "Since our data is limited, so I randomly pick up 10 users whose tweets are more than 5. For each target user, I recommend 5 users for him or her. Because our data is from four years ago, I check whether our recommendations are valuable by checking the current followings of corresponding user.   \n",
    "\n",
    "I list the recommendation results for users as follows and mark & for good recommendations which are really followed by target user now. Then we evaluate the recommend algorithm by this result.  \n",
    "\n",
    "To determine whether one user is following another one, we use Twitter API to determine it. We need to submit request on the console:\n",
    "https://apigee.com/console/twitter\n",
    "\n",
    "The command is as follow:\n",
    "\n",
    "https://api.twitter.com/1.1/friendships/show.json?source_id=[ID_1]&target_id=[ID_2]  \n",
    "\n",
    "Replace the ids of two users in the [ID_1] and [ID_2] as follow and the API will return the result of their relationship.  \n",
    "\n",
    "https://api.twitter.com/1.1/friendships/show.json?source_id=1514379421&target_id=493249666\n",
    "______________________________________________________________________________________________________________________________________________  \n",
    "  \n",
    "Recommendations for user NoticedTheVamps,  user ID:1514379421  \n",
    "user ID - User Name  \n",
    "197759552 - spacehero1995\n",
    "495074577 - TheVampsCon\n",
    "493249666 - TheVampsTristan &\n",
    "30974408 - TheVampsJames &\n",
    "126370504 - TheVampsBrad &  \n",
    "\n",
    "\n",
    "Recommendations for user lawrencedale_,  user ID:529244499  \n",
    "user ID - User Name  \n",
    "158314798 - Real_Liam_Payne &      \n",
    "72064417 - PointlessBlogTv    \n",
    "209708391 - onedirection  &     \n",
    "105119490 - NiallOfficial &    \n",
    "982345171 - ploynthanida  \n",
    "\n",
    "Recommendations for user _Anklebiters_,  user ID:189690616  \n",
    "user ID - User Name  \n",
    "21111883 - ddlovato &    \n",
    "43003845 - paramore &  \n",
    "408774041 - DiannaDeLaGarza  \n",
    "461410856 - TheParamoreBand & \n",
    "40981798 - yelyahwilliams &\n",
    "\n",
    "Recommendations for user louismidnight,  user ID:634024610  \n",
    "user ID - User Name  \n",
    "440205732 - pukestagram  \n",
    "431834660 - hushstyIes  \n",
    "64259689 - harrynstuff &  \n",
    "1425312570 - niallerlust &  \n",
    "525737876 - weyheyygabi &   \n",
    "\n",
    "Recommendations for user unfziam,  user ID:285410994\n",
    "user ID - User Name\n",
    "523753678 - nachoniall\n",
    "158314798 - Real_Liam_Payne &  \n",
    "72064417 - PointlessBlogTv\n",
    "209708391 - onedirection &  \n",
    "105119490 - NiallOfficial &   \n",
    "\n",
    "Recommendations for user FifthharmonyWOW,  user ID:487491990  \n",
    "user ID - User Name  \n",
    "158314798 - Real_Liam_Payne  \n",
    "72064417 - PointlessBlogTv  \n",
    "209708391 - onedirection &  \n",
    "105119490 - NiallOfficial  \n",
    "14268057 - wittynate   \n",
    "\n",
    "Recommendations for user McLeeroyFlurry,  user ID:1334763498  \n",
    "user ID - User Name  \n",
    "956559020 - Updating1DInfo  \n",
    "612924569 - 1DAlert &  \n",
    "267333181 - 1DSuperHumans  \n",
    "477919735 - 1dupdategirls &  \n",
    "591743479 - WW1DUpdates &  \n",
    "\n",
    "Recommendations for user SMASHWORLD_OFC,  user ID:609804455  \n",
    "user ID - User Name  \n",
    "211089590 - SMASHindonesia &  \n",
    "272400533 - ZulviaNazma  \n",
    "326714055 - SMASHFUNIA  \n",
    "257840821 - SMASH_TPI  \n",
    "215245796 - SMASHBANDUNG  \n",
    "\n",
    "Recommendations for user lifeloverforeva,  user ID:1417445364  \n",
    "user ID - User Name\n",
    "158314798 - Real_Liam_Payne &\n",
    "269498821 - tickledpayne\n",
    "105119490 - NiallOfficial &\n",
    "534774305 - ShipNmily &\n",
    "346883535 - OliveiraLais13  \n",
    "\n",
    "Recommendations for user jazzy_jass123,  user ID:827443188  \n",
    "user ID - User Name   \n",
    "308297673 - KianLawley &  \n",
    "45494617 - TrevorMoran  \n",
    "308759071 - SamPottorff &  \n",
    "139539038 - jccaylen &  \n",
    "180949358 - ConnorFranta  \n",
    "\n",
    "\n",
    "**Evaluation**  \n",
    "Total Recommendations: 50   \n",
    "Valid Recommendations: 27  \n",
    "Precision: 54%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Collaborative filtering\n",
    "![](https://lh3.googleusercontent.com/-vcyfyBBpWnjO9Cz2Fb3DetcqAmyGjTpaLFsnGSYuYz9WT0yv3bY8kQFqpm2-A2YnTfGw6DLpAZj4YX9yrRlmaDbazTayzIJ3DaXk3ULGVPb6ccwONPsLMKH0OSl18U_tABA6LMfMHZdztqIXhwjMmd1zq4ZUOoeUH23vzhE6JMf_rgjqiKoWFz-DSuI5cyXu90xPxwX0WiK4UD3egKlDe06SNSAxW68fvGhr6Jk2J03QuY5yRq51uvzNp288eiPgfICiaY0EJ0sOxm1skEcDWIWAIGCQj05RKVewPmBcI4c-_ZIiS3ldHZi15-poei3nwxWvPE9VlqKKDDnT1XvrsN0ZqroKl4IWc_JeJ2Ow8kj-bhmLX9dM3cDUmmCYlQEaXpAHGk5Rtv1qvqOcqEZG8ecrrz1eH2gDmlJA8ZrdFlqFrdVB-qfVXVY7r5BMKIcgVd_ZlLeq6oPBaEjnblDkN00Dshh1wr_nL654vol4__YzaDTyDXoKoYpWWg3ffKfj2fHO4pMUq7ddFgRXC6AIlcM7Iu6WJCL6fNhm8SXj0E_kgxNcu2P-aU2Dqmaz17FQSuhbX1f6RCvbmGxY199fXs4Uxt_PR1KYxyEwpaUDnp8DrTkyV6G=w945-h198-no)\n",
    "\n",
    "### 1) build the graph\n",
    "According to the dataset of following relation http://snap.stanford.edu/data/higgs-social_network.edgelist.gz  \n",
    "We build the followGraph and followedGraph to record all relationship among users\n",
    "\n",
    "**Example**:  \n",
    "A follow B and C  \n",
    "B follow C and D  \n",
    "C follow D and E  \n",
    "  \n",
    "In followGraph, data is presented like this:  \n",
    "{ A->[B,C], B->[C,D], C->[D,E] }  \n",
    "In followedGraph, data is presented like this:  \n",
    "{ B->[A], C->[A,B], D->[B,C], E->[C] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "followMap = {}\n",
    "followedMap = {}\n",
    "userSet = set()\n",
    "with open(\"higgs-social_network.edgelist\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        follow = line.split()\n",
    "        #build followMap\n",
    "        if follow[0] in followMap:\n",
    "            followMap[follow[0]].append(follow[1])\n",
    "        else:\n",
    "            followMap[follow[0]] = [follow[1]]\n",
    "        #build followedMap\n",
    "        if follow[1] in followedMap:\n",
    "            followedMap[follow[1]].append(follow[0])\n",
    "        else:\n",
    "            followedMap[follow[1]] = [follow[0]]\n",
    "        #build userSet\n",
    "        if follow[0] not in userSet:\n",
    "            userSet.add(follow[0])\n",
    "        if follow[1] not in userSet:\n",
    "            userSet.add(follow[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 2) build training data\n",
    "To evaluate our recommender, we need to separate the dataset into two parts, training data and testing data. What we do is separate the first 1000 users' following list and extract the half the following list as the testing data of these users. We will compare the predict followees with these testing data to evaluate our system's accuracy.\n",
    "\n",
    "For example:  \n",
    "A is one of first 1000 users and A follow [B, C, D, E]  \n",
    "then the following list will be separated into two parts, [B, C] (training data) and [D, E] (testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select the first 1000 user as test user\n",
    "# remove half of their followees and use them as predict objects\n",
    "import copy\n",
    "trainFollowMap = copy.deepcopy(followMap)\n",
    "trainFollowedMap = copy.deepcopy(followedMap)\n",
    "\n",
    "n = 0\n",
    "for user in trainFollowMap:\n",
    "    if n < 1000:\n",
    "        followees = trainFollowMap[user]\n",
    "        trainFollowMap[user] = followees[0:len(followees)/2]\n",
    "        for followee in followees[len(followees)/2:]:\n",
    "            trainFollowedMap[followee].remove(user)\n",
    "            if not trainFollowedMap[followee]:\n",
    "                del trainFollowedMap[followee]\n",
    "    else:\n",
    "        break\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3) Implement PageRank on the graph\n",
    "We implement PageRank on the graph to get the popularity rank of users (10 iteration). The popularity is defined as the possibility of being followed by others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pageRank to get popularity rank\n",
    "popularity = {}\n",
    "d = 0.8\n",
    "Num = len(userSet)\n",
    "iterNum = 10\n",
    "for user in userSet:\n",
    "    popularity[user] = 1.0 / len(trainFollowMap)\n",
    "lastPop = popularity.copy()\n",
    "\n",
    "for i in range(iterNum):\n",
    "    sumOfPop = 0\n",
    "    for user in userSet:\n",
    "        if user in trainFollowedMap:\n",
    "            followers = trainFollowedMap[user]\n",
    "        else:\n",
    "            followers = []\n",
    "        popularity[user] = (1-d)/Num\n",
    "        for follower in followers:\n",
    "            popularity[user] += d * lastPop[follower] / len(trainFollowMap[follower])\n",
    "        sumOfPop += popularity[follower]\n",
    "    #normalize popularity\n",
    "    for user in popularity:\n",
    "        popularity[user] /= sumOfPop\n",
    "    lastPop = popularity.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Inverse popularity rank\n",
    "The similariy is defined as the possibility of following the same followees. To find the similar users having similar followee list with the target user, we have a basic idea:  \n",
    "**If two users follow the same followee with lower popularity, the more likely these two are similar**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inverse popularity rank\n",
    "ivs_popularity = {}\n",
    "popOfSum = 0\n",
    "for user in popularity:\n",
    "    ivs_popularity[user] = 1.0/popularity[user]\n",
    "    popOfSum += ivs_popularity[user]\n",
    "for user in popularity:\n",
    "    ivs_popularity[user] /= popOfSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5) Similarity Function\n",
    "As defined above, The similarity of two users is calculated by the cosine of follow vectors and the weight of user is based on the **inverse popularity rank**    \n",
    "\n",
    "We also test the similarity function calculated by **popularity rank** and **jaccard **  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simByIvsPopularity(userA, userB):\n",
    "    common = list(set(trainFollowMap[userA]).intersection(trainFollowMap[userB]))\n",
    "    if not common:\n",
    "        return 0\n",
    "    numerator = 0\n",
    "    for user in common:\n",
    "        numerator += ivs_popularity[user] ** 2\n",
    "    mode = 0\n",
    "    denominator = 1\n",
    "    for user in trainFollowMap[userA]:\n",
    "        mode += ivs_popularity[user] ** 2\n",
    "    denominator *= mode ** 0.5\n",
    "    \n",
    "    mode = 0\n",
    "    for user in trainFollowMap[userB]:\n",
    "        mode += ivs_popularity[user] ** 2\n",
    "    denominator *= mode ** 0.5\n",
    "    return numerator / denominator\n",
    "\n",
    "def simByPopularity(userA, userB):\n",
    "    common = list(set(trainFollowMap[userA]).intersection(trainFollowMap[userB]))\n",
    "    if not common:\n",
    "        return 0\n",
    "    numerator = 0\n",
    "    for user in common:\n",
    "        numerator += popularity[user] ** 2\n",
    "    mode = 0\n",
    "    denominator = 1\n",
    "    for user in trainFollowMap[userA]:\n",
    "        mode += popularity[user] ** 2\n",
    "    denominator *= mode ** 0.5\n",
    "    \n",
    "    mode = 0\n",
    "    for user in trainFollowMap[userB]:\n",
    "        mode += popularity[user] ** 2\n",
    "    denominator *= mode ** 0.5\n",
    "    return numerator / denominator\n",
    "\n",
    "def simJaccard(userA, userB):\n",
    "    common = list(set(trainFollowMap[userA]).intersection(trainFollowMap[userB]))\n",
    "    if not common:\n",
    "        return 0\n",
    "    return len(common) / (len(trainFollowMap[userA]) + len(trainFollowMap[userB]) - len(common))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Evaluation\n",
    "To evaluate the system, we will compare the predict followees with these testing data to evaluate our system's accuracy.  \n",
    "In detail, we selected the top 100 users having highest similarity with the target user and calculate their followees frequency and select top 5 of them as the result of the recommender. We totally test 100 users and get the precision of our recommender system and compare it with other system using other similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId:  378466 1/100\n",
      "userId:  287145 2/100\n",
      "userId:  287146 3/100\n",
      "userId:  287147 4/100\n",
      "userId:  378462 5/100\n",
      "userId:  378463 6/100\n",
      "userId:  287142 7/100\n",
      "userId:  287143 8/100\n",
      "userId:  287148 9/100\n",
      "userId:  378464 10/100\n",
      "userId:  378468 11/100\n",
      "userId:  378469 12/100\n",
      "userId:  370255 13/100\n",
      "userId:  370254 14/100\n",
      "userId:  370257 15/100\n",
      "userId:  378465 16/100\n",
      "userId:  370251 17/100\n",
      "userId:  370250 18/100\n",
      "userId:  370253 19/100\n",
      "userId:  370252 20/100\n",
      "userId:  89378 21/100\n",
      "userId:  89379 22/100\n",
      "userId:  370259 23/100\n",
      "userId:  370258 24/100\n",
      "userId:  287141 25/100\n",
      "userId:  5988 26/100\n",
      "userId:  5989 27/100\n",
      "userId:  378467 28/100\n",
      "userId:  378460 29/100\n",
      "userId:  5982 30/100\n",
      "userId:  5983 31/100\n",
      "userId:  5980 32/100\n",
      "userId:  5981 33/100\n",
      "userId:  5986 34/100\n",
      "userId:  5987 35/100\n",
      "userId:  5984 36/100\n",
      "userId:  5985 37/100\n",
      "userId:  218135 38/100\n",
      "userId:  218134 39/100\n",
      "userId:  287149 40/100\n",
      "userId:  391271 41/100\n",
      "userId:  162929 42/100\n",
      "userId:  89370 43/100\n",
      "userId:  89371 44/100\n",
      "userId:  89372 45/100\n",
      "userId:  391276 46/100\n",
      "userId:  370256 47/100\n",
      "userId:  89374 48/100\n",
      "userId:  73623 49/100\n",
      "userId:  445703 50/100\n",
      "userId:  89375 51/100\n",
      "userId:  89376 52/100\n",
      "userId:  425864 53/100\n",
      "userId:  244765 54/100\n",
      "userId:  89377 55/100\n",
      "userId:  391277 56/100\n",
      "userId:  218138 57/100\n",
      "userId:  16709 58/100\n",
      "userId:  16708 59/100\n",
      "userId:  16705 60/100\n",
      "userId:  16704 61/100\n",
      "userId:  16707 62/100\n",
      "userId:  16706 63/100\n",
      "userId:  16701 64/100\n",
      "userId:  16700 65/100\n",
      "userId:  16703 66/100\n",
      "userId:  16702 67/100\n",
      "userId:  316994 68/100\n",
      "userId:  79232 69/100\n",
      "userId:  316995 70/100\n",
      "userId:  119869 71/100\n",
      "userId:  119868 72/100\n",
      "userId:  353520 73/100\n",
      "userId:  119865 74/100\n",
      "userId:  119864 75/100\n",
      "userId:  119867 76/100\n",
      "userId:  119866 77/100\n",
      "userId:  119861 78/100\n",
      "userId:  119860 79/100\n",
      "userId:  119863 80/100\n",
      "userId:  119862 81/100\n",
      "userId:  316990 82/100\n",
      "userId:  194084 83/100\n",
      "userId:  80279 84/100\n",
      "userId:  80276 85/100\n",
      "userId:  117639 86/100\n",
      "userId:  353521 87/100\n",
      "userId:  80277 88/100\n",
      "userId:  378461 89/100\n",
      "userId:  101350 90/100\n",
      "userId:  80275 91/100\n",
      "userId:  194085 92/100\n",
      "userId:  353269 93/100\n",
      "userId:  353268 94/100\n",
      "userId:  353267 95/100\n",
      "userId:  353266 96/100\n",
      "userId:  353265 97/100\n",
      "userId:  353264 98/100\n",
      "userId:  353263 99/100\n",
      "userId:  353262 100/100\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import operator\n",
    "limit = 100\n",
    "count = 1\n",
    "predictCorrect = 0\n",
    "predictTotal = 0\n",
    "for userA in trainFollowMap:\n",
    "    print \"userId: \",userA,str(count)+ \"/\" + str(limit)\n",
    "    h = []\n",
    "    numOfUser = 100\n",
    "    for userB in trainFollowMap:\n",
    "        if userA != userB:\n",
    "            heapq.heappush(h, (simByIvsPopularity(userA, userB), userB))\n",
    "            if len(h) > numOfUser:\n",
    "                heapq.heappop(h)\n",
    "    followeeList = {}\n",
    "    for e in h:\n",
    "        user = e[1]\n",
    "        for followee in trainFollowMap[user]:\n",
    "            if followee not in trainFollowMap[userA]:\n",
    "                if followee in followeeList:\n",
    "                    followeeList[followee] += 1\n",
    "                else:\n",
    "                    followeeList[followee] = 1\n",
    "    sorted_list = sorted(followeeList.items(), key=operator.itemgetter(1),reverse=True)\n",
    "#     predict = [i[0] for i in sorted_list[0:(len(followMap[userA])+3)/4]]\n",
    "    if len(sorted_list) < 5:\n",
    "        predict = [i[0] for i in sorted_list]\n",
    "    else:\n",
    "        predict = [i[0] for i in sorted_list[0:5]]\n",
    "    real = followMap[userA][len(followMap[userA])/2:]\n",
    "    common = list(set(predict).intersection(real))\n",
    "    predictCorrect += len(common)\n",
    "    predictTotal += len(predict)\n",
    "#     print \"predict: \",predict\n",
    "#     print \"real: \",real\n",
    "#     print \"common: \", common\n",
    "#     print \"accuracy:\", len(common) / float(len(predict))\n",
    "    if count == limit:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "500\n",
      "precision:  0.196\n"
     ]
    }
   ],
   "source": [
    "print predictCorrect\n",
    "print predictTotal\n",
    "print \"precision: \", predictCorrect / float(predictTotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Result\n",
    "We compare the precisions of recommenders with three similarity functions (jaccard, popularity, inverse-popularity) and get the following result:  \n",
    "\n",
    "jaccard: 0.0039603960396  \n",
    "popularity: 0.11  \n",
    "inverse-popularity: 0.196  \n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/2mfidqBWqnGBZx5AopfC5MiRUCK7HTIeSN9B-y-HEl4PEmFAGwViD2Z-zz5_IWGDGY4PPQj_gX26Y7EWCKLxEiApiUiRC2x3pRHDJjJP3cZCyaLjWq9_IC0M0rmFu_jJ9avUVVWMU1YOirnjzTGEgWIqvfHYcY-vWVFfu_AtLi77BsoJN0_5_zKEg09-Lk6yPd0k2oPhgEgrJclgq5d8SUHRqoFx4BZkDR1PwmHy-3q-HvegDDS_TusQ0zF95Zf9kKtOLvJe5aRhQSRjTEwp33rFNCX9Qi9NdpSMgxGEPIibfMKMC2RfcwIGExkVYTaropvyssvgJJISxO9f06odJ9r-KQqil3IiEvRB7olE1XcWIqNIhI-MUvLSGCPC_JREqRG0dWXhlILzzwr5X86dP8md-FxrMU4-Sz8Cg7SEsKxcufMXEdDEtxlnHMHAE1vpdddBJs9FIvaxK6zqbFXqvxnRUv_DxpsBrgi_qvEMIg6sLUHzJSMMzWzOfnyVwP7izr6rw7XO-ak78f3LJEMu0eSF3_ppZ4Cc-ZkBpPZopUrL5lQ_VB602G0bwiBjfysDEoZEjQGeljQoimr0Ryn6rBL11M3ZI2uawpujTNhUeQhBJlg61xYu=w929-h535-no\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion:\n",
    "In short, for recommender system, users’ hobbies and interests are indeed a hard part to predict. In the collaborative filtering part, we found that if two people concerned about similar topics, they actually would not follow each other. However, if their interests are similar, it makes sense that the rest followings of one user is interesting for the other one, which is just like what we learn for recommending movies. We use different methods to calculate similarities of users, such as jaccard and cosine, and find out our best way to solve this problem, the inverse popularity ranking. \n",
    "\n",
    "For the hybrid content-based part, we found that the baseline is very important for recommendation. If a user mentioned one people in a topic, what that guy concerned about would also mean a lot to the target use. And by observation and experiment, we also found that it’s not true that the more popular a user is, the more likely it would be followed.The user who is more likely followed is always a normal user which means the number of his or her followers is usually in a certain range around 50-150. And ignoring stop word and URL in user’s posts also helps to promote our accuracy. For this part, we use tokenization technique to preprocess tweets and pagerank algorithm to calculate the baseline of recommendation score. And adjusting our parameters based on the results of experiments.\n",
    "\n",
    "Due to the limitation of data, our result is not as good as what is said in paper. But we just wanna try to realize and add some of our ideas in the existing algorithm and then figure out what we can do. \n",
    "\n",
    "For the next step, we think we will redefine each user with their profile and improve the similarity algorithm and mark every one with a topic by semantic analysis if with more tweets data for the content-based part.\n",
    "\n",
    "## Reference\n",
    "[1] Page L, Brin S, Motwani R, Winograd T. The PageRank citation ranking: Bringing order to the web. Stanford InfoLab; 1999 Nov 11.  \n",
    "[2] Breese, John S., David Heckerman, and Carl Kadie. \"Empirical analysis of predictive algorithms for collaborative filtering.\" Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1998.  \n",
    "[3] Hannon, John, Mike Bennett, and Barry Smyth. \"Recommending twitter users to follow using content and collaborative filtering approaches.\" Proceedings of the fourth ACM conference on Recommender systems. ACM, 2010.   \n",
    "[4] Lops, Pasquale, Marco De Gemmis, and Giovanni Semeraro. \"Content-based recommender systems: State of the art and trends.\" Recommender systems handbook. Springer US, 2011. 73-105.  \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
